### LABEL 1(이상치)의 비율
```python
label_1_ratio = con_df['LABEL'].mean()

print(f"LABEL 1 비율: {label_1_ratio * 100:.2f}%")
# LABEL 1 비율: 18.18%
```


## 1. Isolation Forest

### 튜닝
* 해당 Isolation Forest를 5폴드 교차검증한 결과 F1 Score가 가장 높은 최적의 오염 비율은 0.14였다. 
* 하지만 이상거래 탐지에 있어서 Recall이 높은 값이 중요하기 때문에 Recall이 높으면서, F1 Score가 높은 값을 튜닝해본 결과 오염 비율을 23%로 조정하는것이 낫다고 판단되어 23%로 수정하였다.

```python
#isolation forest

import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.metrics import accuracy_score, classification_report,precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
 

# 정상 데이터와 허위 데이터를 결합
full_df = con_df.copy()

# REVIEW_TIME을 float로 변환하여 타임스탬프로 저장
full_df['REVIEW_TIME_TS'] = pd.to_datetime(full_df['REVIEW_TIME']).apply(lambda x: x.timestamp())

# 기존 features에 REVIEW_TIME_TS를 추가
features = [ 'SHOP_ID','CUST_ID', 'REVIEW_TIME_TS',"REVIEW_RANK"]

# Isolation Forest 모델 훈련 (Review Time 포함)
# 정확도를 위해
model = IsolationForest(contamination=0.23, random_state=42) # 23%의 이상치 비율로 설정

full_df['predicted_anomaly'] = model.fit_predict(full_df[features])

# 이상치(-1)와 정상치(1)를 구분하여 predicted_anomaly 열에 표시
full_df['predicted_anomaly'] = full_df['predicted_anomaly'].apply(lambda x: 1 if x == -1 else 0)

# 정확도 평가
accuracy = accuracy_score(full_df['LABEL'], full_df['predicted_anomaly'])

precision = precision_score(full_df['LABEL'], full_df['predicted_anomaly'])

recall = recall_score(full_df['LABEL'], full_df['predicted_anomaly'])

f1 = f1_score(full_df['LABEL'], full_df['predicted_anomaly'])

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')


# predicted_anomaly와 동일
full_df['iso_pred'] = model.fit_predict(full_df[features])
full_df['iso_pred'] = full_df['iso_pred'].apply(lambda x: 1 if x == -1 else 0)
```

<br>

### 결과
Accuracy: 0.8629  <br>
Precision: 0.5971 <br>
Recall: 0.7556  <br>
F1 Score: 0.6671 <br>

<br>

## 2. Auto Encoder

### 튜닝
#### 1. 데이터 정규화 방식 변경
- **기존 코드**: `StandardScaler`를 사용하여 데이터를 정규화.
    - `StandardScaler`는 데이터를 **평균 0, 표준편차 1**로 변환한다. 이것은 정규 분포를 따르는 데이터에 적합하지만, 비정규 분포 데이터에서는 모델 학습이 어려울수 있다.
- **수정된 코드**: `MinMaxScaler`를 사용하여 데이터를 **0과 1 사이**의 값으로 정규화.
    - **MinMaxScaler**는 신경망 기반 모델(특히 `AutoEncoder`)에 더 적합하며, 비정규 분포의 데이터도 더 쉽게 학습할 수 있도록 도와준다.

#### 2. 잠재 공간 크기 (encoding_dim) 조정
- **기존 코드**: 잠재 변수 크기(`encoding_dim`)를 **3**으로 설정.
    - 잠재 공간의 크기가 너무 작으면 데이터의 중요한 정보를 충분히 담지 못할수있다.
- **수정된 코드**: 잠재 변수 크기를 **6**으로 증가.
    - 잠재 공간의 크기를 적절히 키움으로써 데이터 표현력이 향상될 수 있다. 너무 크면 과적합의 위험이 있다.
#### 3. AutoEncoder 모델 구조 변경

- **기존 코드**: 간단한 AutoEncoder 구조
    - 모델이 지나치게 간단하여 학습 능력이 제한될 수 있다.
- **수정된 코드**: 더 깊은 AutoEncoder 구조로 개선.
    - **128개 노드를 가진 레이어**를 인코딩과 디코딩 과정에 추가하여 모델의 복잡도를 늘리고, 더 많은 패턴을 학습할 수 있게 변경.

### 4. 훈련 파라미터 조정
- **기존 코드**: `epochs=50`, `batch_size=32`로 설정.
    - 학습 기간이 짧아 모델이 충분히 학습되지 않을 수 있다.
- **수정된 코드**: `epochs=100`, `batch_size=64`로 증가.
    - 더 긴 학습 기간과 큰 배치 크기를 설정하여 모델이 데이터를 더 충분히 학습할 수 있도록 했다.

#### 5. 재구성 오류 계산 방식 변경
- **기존 코드**: **MSE(Mean Squared Error)**와 **코사인 유사도**를 결합한 방식으로 이상치 점수를 계산.
    - 코사인 유사도는 벡터 간의 각도를 기준으로 유사성을 측정하는데, 이 방식이 유사성이 낮을 때도 거리 차이를 반영하지 못할 수 있다.
- **수정된 코드**: **MSE**와 **유클리드 거리(Euclidean Distance)**를 결합하여 이상치 점수 계산.
    - 유클리드 거리는 벡터 간의 실제 거리를 측정하므로, 원본 데이터와 재구성된 데이터 간의 차이를 더 명확히 반영할 수 있다.

#### 6. 모델 성능 평가 방식
- **기존 코드**: **코사인 유사도**와 **MSE** 기반 이상치 점수를 사용.
- **수정된 코드**: **MSE**와 **유클리드 거리** 기반 이상치 점수를 사용하여 모델이 원본 데이터와의 차이를 더 잘 반영하도록 개선.

### 7. 모델 성능 평가 지표 (Accuracy, Precision, Recall, F1 Score)

- 두 코드 모두 성능 평가 지표로 **Accuracy**, **Precision**, **Recall**, **F1 Score**를 사용하여 모델의 성능을 평가하고, 이를 바탕으로 최적의 **임계값(threshold)**을 선택한다.

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import tensorflow as tf
from tensorflow.keras import layers, models

  
# 데이터 로드 및 전처리
full_df2 = con_df.copy()
full_df2['REVIEW_TIME'] = full_df2['REVIEW_TIME'].apply(lambda x: int(pd.Timestamp(x).timestamp()))
  

# 피처 설정
features = full_df2[['SHOP_ID', 'CUST_ID', 'REVIEW_RANK', 'REVIEW_TIME']].values

  
# 데이터 정규화 (MinMaxScaler로 변경)
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)

  

# AutoEncoder 모델 정의
input_dim = features_scaled.shape[1]
encoding_dim = 6 # 잠재 변수의 크기를 늘림 (임의로 6로 설정)
input_layer = layers.Input(shape=(input_dim,))
encoded = layers.Dense(128, activation='relu')(input_layer)
encoded = layers.Dense(encoding_dim, activation='relu')(encoded)
decoded = layers.Dense(128, activation='relu')(encoded)
decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)

autoencoder = models.Model(inputs=input_layer, outputs=decoded)
autoencoder.compile(optimizer='adam', loss='mse')

  

# 정상 데이터만을 사용하여 모델 학습
normal_data = features_scaled[full_df2['LABEL'] == 0]
X_train, X_val = train_test_split(normal_data, test_size=0.2, random_state=42)
autoencoder.fit(X_train, X_train, epochs=100, batch_size=64, validation_data=(X_val, X_val), verbose=1)

  

# 전체 데이터에 대한 재구성
reconstructed = autoencoder.predict(features_scaled)

# 재구성 오류 계산 (MSE)
reconstruction_error = np.mean(np.square(features_scaled - reconstructed), axis=1)

# 유클리드 거리 계산
euclidean_dist = np.linalg.norm(features_scaled - reconstructed, axis=1)

# 이상치 점수 계산 (유클리드 거리 사용)
anomaly_score = reconstruction_error + euclidean_dist

# 임계값 설정 및 평가
thresholds = np.linspace(min(anomaly_score), max(anomaly_score), 50)
precision_scores = []
recall_scores = []
f1_scores = []


for threshold in thresholds:
	predicted_anomalies = anomaly_score > threshold
	precision_scores.append(precision_score(full_df2['LABEL'], predicted_anomalies))
	recall_scores.append(recall_score(full_df2['LABEL'], predicted_anomalies))
	f1_scores.append(f1_score(full_df2['LABEL'], predicted_anomalies))

# 최적의 임계값 선택
best_threshold_index = np.argmax(f1_scores)
best_threshold = thresholds[best_threshold_index]
best_f1 = f1_scores[best_threshold_index]

# 최적의 임계값 적용
predicted_anomalies = anomaly_score > best_threshold
full_df2['an_anomaly'] = predicted_anomalies

# 성능 평가
accuracy = accuracy_score(full_df2['LABEL'], full_df2['an_anomaly'])
precision = precision_score(full_df2['LABEL'], full_df2['an_anomaly'])
recall = recall_score(full_df2['LABEL'], full_df2['an_anomaly'])
f1 = f1_score(full_df2['LABEL'], full_df2['an_anomaly'])

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print(f"k_threshold: {best_threshold}")
```


<br>

### 결과
Accuracy: 0.9201  <br>
Precision: 0.8738  <br>
Recall: 0.6549  <br>
F1 Score: 0.7486 <br>
k_threshold: 0.018666693635481547 <br>


<br>

## 3. XGBoost
### 튜닝
####  1. 모델 비교
#### XGBoost:

- **기본 개념**: XGBoost는 **부스팅(Boosting)** 기법을 사용하여 이전 트리에서 잘못 예측된 부분을 보완하면서 점진적으로 모델을 개선하는 방식이다.
- **주요 하이퍼파라미터**:
    - `max_depth`: 트리의 최대 깊이. 깊이를 줄이면 과적합을 방지할 수 있다.
    - `min_child_weight`: 트리가 분기할 수 있는 최소 가중치로, 과적합 방지에 사용된다.
    - `subsample`: 데이터를 샘플링하는 비율로, 값이 1보다 작으면 일부 데이터만 사용하여 모델을 학습하므로 과적합을 방지할 수 있다.
    - `colsample_bytree`: 각 트리에서 사용할 피처의 비율로, 트리마다 다른 피처를 사용하게 하여 다각적인 모델을 만들 수 있다.
- **장점**:
    - 과적합 방지 기능이 잘 탑재되어 있어 복잡한 모델을 생성하더라도 좋은 일반화 성능을 기대할 수 있다.
    - 계산 효율성이 높아 큰 데이터셋에서도 좋은 성능을 보인다.
- **단점**:
    - **작은 데이터셋에서는 과하게 복잡한 모델이 될 가능성이 있다.**

#### 랜덤 포레스트:

- **기본 개념**: 랜덤 포레스트는 **배깅(Bagging)** 기법을 사용하여 여러 개의 트리를 독립적으로 학습하고 그 결과를 평균 내거나 다수결로 예측을 한다.
- **주요 하이퍼파라미터**:
    - `n_estimators`: 생성할 트리의 개수. 트리 개수를 늘리면 성능이 안정되지만 계산 비용이 증가할 수 있다.
    - `max_depth`: 트리의 최대 깊이. 깊이를 조정하여 과적합을 방지할 수 있다.
- **장점**:
    - 각 트리가 독립적으로 학습되므로 병렬 학습이 가능하며, 일반적으로 **과적합에 강한 모델**로 알려져 있다.
    - 작은 데이터셋에서도 좋은 성능을 발휘할 수 있다.
- **단점**:
    - 많은 트리를 생성해야 하므로 계산 비용이 높아질 수 있으며, 데이터가 매우 크거나 복잡할 때는 XGBoost보다 성능이 떨어질 수 있다.

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split

  

# 데이터 준비
features = full_df4[['SHOP_ID', 'CUST_ID', 'REVIEW_RANK', 'REVIEW_TIMESTAMP']]
labels = full_df4['LABEL']

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)

# 랜덤 포레스트 모델
rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
rf_model.fit(X_train, y_train) 

# 예측
y_pred = rf_model.predict(X_test)

# 평가
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# 결과 출력
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
```


<br>

### 결과
Accuracy: 0.9443 <br>
Precision: 0.9745  <br>
Recall: 0.7316 <br>
F1 Score: 0.8358 <br>

<br>

## 4. Keras Sequantial Model
### 튜닝

#### 1. Huber Loss 사용

- **기존 코드**: MSE(Mean Squared Error) 손실 함수를 사용한다. MSE는 재구성 오류를 계산할 때 사용되며, 이상치에 민감하여 이상치가 큰 오류를 유발할 수 있다.
- **개선된 코드**: `HuberLoss`가 사용되었다. Huber 손실은 MSE와 MAE(Mean Absolute Error)의 장점을 결합한 방식으로, 이상치에 대해 덜 민감하고, 그로 인해 더 안정적인 학습을 가능하게 한다. 이상치가 모델 학습에 미치는 영향을 줄이기 때문에 재구성 오류 기반 이상 탐지 모델에서 더 좋은 성능을 발휘할 가능성이 있다.

#### 2. Early Stopping 적용

- **기존 코드**: 학습 과정에서 Early Stopping 기법이 적용되지 않았다. 따라서 고정된 에포크 수만큼 학습을 진행했으며, 과적합이 발생할 가능성이 있다.
- **개선된 코드**: `Early Stopping`을 추가, Early Stopping은 학습 과정에서 검증 손실이 개선되지 않는 경우, 과적합을 방지하기 위해 학습을 조기에 중단하는 기법이다. 이를 통해 불필요한 학습을 줄이고, 최적의 성능이 나올 때 학습을 중단하여 일반화 성능을 개선할 수 있다.

####  추가 개선 요소
- **LeakyReLU 활성화 함수 사용**: ReLU 대신 LeakyReLU가 적용되어, 뉴런이 죽는 문제를 해결하고 더 풍부한 표현 학습을 가능하게 한다.
- **Dropout 비율 조정**: `Dropout` 비율이 적절히 조정되어 과적합을 방지한다.
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import precision_recall_curve, accuracy_score, precision_score, recall_score, f1_score
import numpy as np
import pandas as pd

  
# 데이터 로드 및 전처리
full_df3 = con_df.copy()
full_df3['REVIEW_TIME'] = pd.to_datetime(full_df3['REVIEW_TIME'])
full_df3['REVIEW_TIMESTAMP'] = full_df3['REVIEW_TIME'].apply(lambda x: x.timestamp())

  
# 피처와 레이블 설정
features = full_df3[['SHOP_ID', 'CUST_ID', 'REVIEW_RANK', 'REVIEW_TIMESTAMP']].values
labels = full_df3['LABEL'].values # 실제 레이블

  
# 데이터 정규화 (MinMaxScaler 사용)
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)

  
# PyTorch 텐서로 변환
data = torch.tensor(features_scaled, dtype=torch.float32)
labels_tensor = torch.tensor(labels, dtype=torch.int64)

  
# DataLoader로 변환
batch_size = 16
dataset = TensorDataset(data)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

  
# AutoEncoder 모델 정의
class Autoencoder(nn.Module):
	def __init__(self, input_dim, encoding_dim):
		super(Autoencoder, self).__init__()
		# Encoder
		self.encoder = nn.Sequential(
		nn.Linear(input_dim, 128),
		nn.LeakyReLU(),
		nn.Dropout(0.2), # 드롭아웃 비율 조정
		nn.Linear(128, encoding_dim),
		nn.LeakyReLU()
		)

		# Decoder
		self.decoder = nn.Sequential(
		nn.Linear(encoding_dim, 128),
		nn.LeakyReLU(),
		nn.Dropout(0.2),
		nn.Linear(128, input_dim),
		nn.Sigmoid() # 입력 데이터가 [0, 1]로 정규화되어 있다고 가정
		)
		
		  

	def forward(self, x):
		encoded = self.encoder(x)
		decoded = self.decoder(encoded)
		return decoded

  
# 모델 인스턴스 생성
input_dim = data.shape[1] # 입력 데이터의 차원 (SHOP_ID, CUST_ID, REVIEW_RANK, REVIEW_TIMESTAMP)
encoding_dim = 5 # 인코딩 차원
model = Autoencoder(input_dim, encoding_dim)

  
# 손실 함수와 옵티마이저 정의
criterion = nn.HuberLoss() # HuberLoss 사용
optimizer = optim.Adam(model.parameters(), lr=0.0005) # 학습률 조정



# Early Stopping 설정
best_loss = float('inf')
patience = 10 # patience 설정 (연속적으로 개선되지 않으면 학습을 중단)
trigger_times = 0


# 모델 학습
num_epochs = 100
  

for epoch in range(num_epochs):
	model.train()
	running_loss = 0.0
	for batch in dataloader:
		inputs = batch[0]
	
		# Forward pass
		outputs = model(inputs)
		loss = criterion(outputs, inputs)

		# Backward pass and optimization
		optimizer.zero_grad()
		loss.backward()
		optimizer.step()
		
		running_loss += loss.item()

  

	epoch_loss = running_loss / len(dataloader)
	print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')

  
# Early Stopping 체크
if epoch_loss < best_loss:
	best_loss = epoch_loss
	trigger_times = 0
else:
	trigger_times += 1
	if trigger_times >= patience:
		print("Early stopping activated.")
		break

  

# 이상 탐지

model.eval()

  
# 모든 데이터를 사용하여 재구성 오류 계산
with torch.no_grad():
	reconstructed = model(data)
	reconstruction_error = torch.mean((data - reconstructed) ** 2, dim=1).numpy()


# Precision-Recall 커브를 사용한 임계값 선택
precision, recall, thresholds = precision_recall_curve(labels, reconstruction_error)


# NaN 방지: 0으로 나눌 때 발생하는 에러 방지
f1_scores = np.divide(2 * (precision * recall), (precision + recall), where=(precision + recall) != 0)
f1_scores = np.nan_to_num(f1_scores) # NaN을 0으로 변환

  
# 최적의 임계값 선택
best_threshold_index = np.argmax(f1_scores)
best_threshold = thresholds[best_threshold_index]
best_f1 = f1_scores[best_threshold_index]


# 임계값 적용
predicted_anomalies = (reconstruction_error > best_threshold).astype(int)

  
# 평가 지표 계산
accuracy = accuracy_score(labels, predicted_anomalies)
precision = precision_score(labels, predicted_anomalies)
recall = recall_score(labels, predicted_anomalies)
f1 = f1_score(labels, predicted_anomalies)


# 결과 출력
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Best Threshold: {best_threshold}")

  
# 예측 결과를 원본 데이터에 추가
full_df3['p_auto_pred'] = predicted_anomalies

```


<br>

### 결과
Accuracy: 0.7993  <br>
Precision: 0.4602  <br>
Recall: 0.6018 <br>
F1 Score: 0.5215 <br>
Best Threshold: 6.380151171470061e-05 <br>

