

## 함수

1. remove_special_characters() : 특수 문자 및 이모지 제거 함수
3. initialize_db() : 데이터베이스 연결 및 초기화 함수
4. initialize_webdriver() : 웹드라이버 초기화 함수
5. navigate_to_finance_section(driver):  증권 주제로 이동 및 뉴스 보기 설정 함수 
6. get_news_page_dates(driver, four_day_ago) : 뉴스 페이지 날짜 가져오기 함수
7.  crawl_news_data(driver) : 뉴스 데이터 크롤링 함수
8. save_news_data(cursor, news_data) : 뉴스 데이터 저장 함수
9. main() : 메인 함수

### 각 함수의 역할
#### remove_special_characters()
한글, 영문, 숫자, 공백을 제외한 특수 문자 및 이모지를 제거하는 함수입니다.
DB에 값을 삽입하기 위해서 데이터 인코딩 오류를 방지하기 위해 작성되었습니다.

<br>

#### initialize_db()
MariaDB 연결 설정 및 커서를 반환하는 함수입니다.
데이터베이스의 연결을 위한 설정값이 선언되어 있으며, 설정값을 전달해주는 역할을 합니다.

<br>

#### initialize_webdriver()
웹 드라이버를 초기화하고 한경닷컴의 모든 뉴스 페이지에 접속하는 역할을 합니다.
웹 스크래핑 또는 데이터 수집 작업을 수행하는 데 필요한 준비 단계입니다.

<br>

#### navigate_to_finance_section(driver)
한경 닷컴의 뉴스 기사 주제들 중 '증권' 주제로 이동 및 한경 뉴스만 볼 수 있도록 설정하는 함수입니다.

<br>


#### get_news_page_dates(driver, four_day_ago)
주어진 드라이버와 4일 전의 날짜를 사용하여 한경닷컴의 모든 뉴스 페이지를 탐색하고, 각 페이지에서 날짜를 추출합니다.
웹 스크래핑 작업에서 4일 전까지의 뉴스를 가져오는 데 사용됩니다.

1. **뉴스 페이지 탐색**: 주어진 드라이버를 사용하여 한경닷컴의 뉴스 페이지에 접속합니다.
2. **더보기 버튼 클릭**: 페이지에서 더보기 버튼을 찾아 클릭합니다. 이를 통해 페이지에 더 많은 뉴스가 로드됩니다.
3. **날짜 추출**: 페이지에서 모든 뉴스 요소를 찾고, 각 요소에서 날짜를 추출합니다.
4. **4일 전 날짜 확인**: 추출한 날짜를 리스트에 추가한 후, 4일 전의 날짜가 리스트에 있는지 확인합니다.
5. **종료 또는 계속 진행**: 4일 전 날짜가 발견되면 뉴스 페이지의 날짜 목록을 반환합니다. 그렇지 않으면 더 많은 페이지를 탐색합니다.

<br>

#### crawl_news_data(driver)
웹 페이지에서 뉴스 데이터를 크롤링하여 반환하는 역할을 합니다.

1. **뉴스 아이템 탐색**: 함수는 주어진 드라이버에서 CSS 선택자를 사용하여 모든 뉴스 아이템을 찾습니다. 각각의 뉴스 아이템은 뉴스의 제목, 날짜, URL, 호스트, 이미지 및 텍스트로 구성됩니다.
2. **뉴스 데이터 추출**: 각 뉴스 아이템에서 제목, 날짜, URL, 호스트, 이미지 및 텍스트를 추출합니다. 이 때, `remove_special_characters` 함수를 사용하여 특수 문자를 제거하고, 날짜 형식을 변환합니다.
3. **데이터 구조화**: 추출한 데이터를 튜플의 형태로 구조화하고, 이를 `news_data` 리스트에 추가합니다.
4. **데이터 반환**: 모든 뉴스 아이템에 대한 데이터가 `news_data` 리스트에 저장되면, 이를 반환합니다.

<br>

#### save_news_data(cursor, news_data) 
뉴스 데이터를 SQL에 담아, 데이터베이스에 저장하는 함수입니다.
text가 공백일 경우에 해당 데이터는 건너뛰기 되어 저장하지 않습니다.

<br>

#### main() 
해당 프로그램의 주요 동작을 정의하고 실행합니다.
해당 함수에서 정의되고 실행하는 역할은 다음과 같습니다.

1. **현재 날짜와 4일 전 날짜 가져오기**: 현재 날짜를 가져오고, 이를 기반으로 4일 전의 날짜를 계산합니다. 이 날짜는 한경닷컴에서 가져올 뉴스 데이터의 범위를 설정하는 데 사용됩니다.
2. **데이터베이스 및 웹드라이버 초기화**: `initialize_db()` 함수를 호출하여 데이터베이스 연결을 초기화하고, `initialize_webdriver()` 함수를 호출하여 웹 드라이버를 초기화합니다.
3. **증권 섹션으로 이동**: `navigate_to_finance_section()` 함수를 호출하여 한경닷컴에서 증권 섹션으로 이동합니다.
4. **뉴스 페이지 날짜 가져오기**: `get_news_page_dates()` 함수를 호출하여 4일 전까지의 뉴스 페이지의 날짜를 가져옵니다. 
5. **뉴스 데이터 크롤링**: `crawl_news_data()` 함수를 호출하여 뉴스 데이터를 크롤링합니다. 
6. **데이터베이스에 뉴스 데이터 저장**: `save_news_data()` 함수를 호출하여 크롤링한 뉴스 데이터를 데이터베이스에 저장합니다. 
7. **종료**: 데이터베이스 연결을 닫고, 웹 드라이버를 종료합니다.

<br>

## DB TABLE 내용
![](https://i.imgur.com/7JI6jev.png)
1. id : 고유 번호, primary key
2. title : 뉴스 제목, varchar 
3. date : 뉴스 날짜, date
4. url : 뉴스가 있는 주소, varchar
5. host : 뉴스가 크롤링 된 사이트, varchar
6. img : 이미지 주소, varchar
7. docsent : None, varchar
8. sentscore : None, int
9. text : 뉴스 내용, varchar


## 파이썬 코드
```python
import requests
import pymysql
import time
from datetime import datetime, timedelta
from fake_useragent import UserAgent
from urllib.parse import urlparse
from selenium import webdriver
from selenium.webdriver.common.by import By
import re

# 사용자 에이전트 설정
ua = UserAgent()
headers = {
    "User-Agent": ua.random
}


def remove_special_characters(text):
   # 한글, 영문, 숫자, 공백을 제외한 특수 문자 및 이모지 제거
    text = re.sub(r'[^\w\s\u3131-\uD79D]', '', text)
    return text
    
def initialize_db():
    # MariaDB 연결 및 커서 반환
    db = pymysql.connect(
        host='localhost',
        port=3306,
        user='root',
        passwd='jen401018&',
        db='news_db',
        charset='utf8'
    )
    return db, db.cursor()
    
def initialize_webdriver():
    # 웹드라이버 초기화 및 반환
    driver = webdriver.Chrome()
    driver.get("https://www.hankyung.com/all-news")
    time.sleep(2)
    return driver
    
def navigate_to_finance_section(driver):
    # 증권 주제로 이동 및 한경 뉴스만 보기 설정
    driver.find_element(By.CSS_SELECTOR, "#inner > header > div > div.section__gnb__wrap > nav > ul > li:nth-child(6) > a").click()
    time.sleep(2)
    driver.find_element(By.CSS_SELECTOR, "#container > div.all-news-wrap.layout-inner > div.contents > div.module-group.box-module.slick-initialized.slick-slider > div > div > div.box-module-inner.financeDiv.slick-slide.slick-current.slick-active > div.box-tit-area > div > button").click()
    time.sleep(2)
    
def get_news_page_dates(driver, four_day_ago):
    # 4일 전까지 뉴스 페이지 날짜 가져오기
    news_page_dates = []
    while True:
        more_button = driver.find_element(By.CSS_SELECTOR, "#container > div.all-news-wrap.layout-inner > div.contents > div.module-group.box-module.slick-initialized.slick-slider > div > div > div.box-module-inner.financeDiv.slick-slide.slick-current.slick-active > div.btn-more-wrap > button")
        more_button.click()
        time.sleep(3)
        news_elements = driver.find_elements(By.CSS_SELECTOR, "#container > div.all-news-wrap.layout-inner > div.contents > div.module-group.box-module.slick-initialized.slick-slider > div > div > div.box-module-inner.financeDiv.slick-slide.slick-current.slick-active > div.daily-news > div")
        for element in news_elements:
            news_date_text = element.find_element(By.TAG_NAME, "strong").text
            news_page_dates.append(news_date_text)
            if four_day_ago in news_page_dates:
                return news_page_dates
        else:
            continue
            
def crawl_news_data(driver):
    # 뉴스 데이터 크롤링
    news_items = driver.find_elements(By.CSS_SELECTOR, ".daily-news .news-list li")
    news_data = []
    for item in news_items:
        title = remove_special_characters(item.find_element(By.CSS_SELECTOR, 'h3.news-tit a').text.strip().replace("'", "''").replace('"', ''))
        date = item.get_attribute('data-aid')[:8]
        date = datetime.strptime(date, '%Y%m%d').strftime('%Y-%m-%d')
        url = item.find_element(By.CSS_SELECTOR, 'h3.news-tit a').get_attribute('href')
        parsed_url = urlparse(url)
        host = remove_special_characters(parsed_url.netloc.replace("'", "''"))
        img_elements = item.find_elements(By.CSS_SELECTOR, 'div.thumb img')
        img = img_elements[0].get_attribute('src') if img_elements else ""
        text = remove_special_characters(item.find_element(By.CSS_SELECTOR, 'p.lead').text.strip().replace("'", "''").replace('"', ''))
        news_data.append((title, date, url, host, img, text))
    return news_data
    
def save_news_data(cursor, news_data):
    # 뉴스 데이터 저장
    for data in news_data:
        title, date, url, host, img, text = data
        if text:
            sql = f"""
            INSERT INTO news_data (id, title, date, url, host, img, docsent, sentscore, text)
            VALUES (NULL,'{title}', '{
![](https://i.imgur.com/eLbyCju.png)
date}', '{url}', '{host}', '{img}', NULL, NULL, '{text}');
            """
            cursor.execute(sql)
        else:
            continue
            
def main():
    # 현재 날짜와 4일 전 날짜 가져오기
    now = datetime.today()
    four_day = now - timedelta(4)
    four_day_ago = four_day.strftime("%Y.%m.%d")
    db, cursor = initialize_db()
    driver = initialize_webdriver()
    try:
        navigate_to_finance_section(driver)
        get_news_page_dates(driver, four_day_ago)
        news_data = crawl_news_data(driver)
        save_news_data(cursor, news_data)
        db.commit()
    except Exception as e:
        print(f"Error: {e}")
        db.rollback()
    finally:
        cursor.close()
        db.close()
        driver.quit()
if __name__ == "__main__":
    main()
```


<br>

## 실행결과

![](https://i.imgur.com/lpXbh6t.png)
